{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploration_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YDqvD25jevzsy6qRY6pJVLFlPxyYDEIn",
      "authorship_tag": "ABX9TyPx7qN+YbCxjzKPtgJCLJ9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dong-uk-kim97/Exploration/blob/main/Exploration_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "vPFUJP72bIKi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7kS93wt04nMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b26f6a-c9d5-4e1b-b51c-25a874556ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\", \"Let's just say that maybe\", 'You could help me ease my mind', \"I ain't Mr. Right But if you're looking for fast love\", \"If that's love in your eyes\", \"It's more than enough\", 'Had some bad love', \"So fast love is all that I've got on my mind Ooh, ooh\", 'Ooh, ooh Looking for some affirmation', 'Made my way into the sun', 'My friends got their ladies', \"And they're all having babies\", \"I just wanna have some fun I won't bore you with the details, baby\", \"I don't even wanna waste your time\", \"Let's just say that maybe\", 'You could help me ease my mind']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import glob, re\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/lyricist/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정제"
      ],
      "metadata": {
        "id": "hNwjYowxb7dZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "원하는 문장만 출력"
      ],
      "metadata": {
        "id": "jE7s0ZrTcgNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= []\n",
        "\n",
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue           # 길이가 0인 문장 제외\n",
        "    # if ('[' and ']') in sentence: continue  # 대괄호가 있는 문장을 제외하려고 했으나 안 좋은 결과를 얻었다.\n",
        "    if sentence[-1] == \":\": continue          # 문장 마지막에 ':'로 끝나는 문장 제외\n",
        "    # if len(sentence.split()) > 20 :continue # maxlen으로 줄여야 했는데 그걸 모르고 이 조건을 사용했더니 안 좋은 결과를 얻었다.\n",
        "    \n",
        "    corpus.append(sentence)\n",
        "\n",
        "\n",
        "print(f'원래 데이터 크기 : {len(raw_corpus)}')\n",
        "print(f'공백인 문장, 뒤에 \\\":\\\" 으로 끝나는 문장 제외한 데이터 크기 : {len(corpus)}')\n",
        "\n",
        "corpus = list(set(corpus))          # 중복불가한 set의 특성을 이용하여 중복되는 문장 제외 후 다시 리스트로 변환\n",
        "print(f'중복되는 문장 제외한 데이터 크기 : {len(corpus)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awu9VYmU155M",
        "outputId": "2ef13ba2-b7ee-41f7-8f43-40f51bd34e8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 데이터 크기 : 187088\n",
            "공백인 문장, 뒤에 \":\" 으로 끝나는 문장 제외한 데이터 크기 : 175749\n",
            "중복되는 문장 제외한 데이터 크기 : 117960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 정의\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()                     # 1\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
        "    sentence = sentence.strip()                             # 5\n",
        "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "bO2XDaQO2D74"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 데이터를 모은다.\n",
        "corpus_tokenized = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    # 데이터 정제\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus_tokenized.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인\n",
        "corpus_tokenized[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4h09omu2Ewe",
        "outputId": "f14c2dde-b713-4cf5-e523-5b9cc3230978"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> don t be afraid to tell me you re sad <end>',\n",
              " '<start> u oughta let me come and pet u so it lasts , baby <end>',\n",
              " '<start> funny how quick these pricks forget <end>',\n",
              " '<start> they ve never seen a fire this hot <end>',\n",
              " '<start> i fucked your mom <end>',\n",
              " '<start> that from you or me <end>',\n",
              " '<start> and wear a shield <end>',\n",
              " '<start> word on the street somebody put a hit on me <end>',\n",
              " '<start> bitch if you died , wouldn t buy you life <end>',\n",
              " '<start> i ve been down <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화"
      ],
      "metadata": {
        "id": "lZjcqaD_b_it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=14000,         # 14000개 단어를 기억할 수 있음\n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"        # 포함되지 않는 단어는 <unk> 으로 표현\n",
        "    )\n",
        "    \n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    \n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    \n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=20)  # 토큰 20개 초과 제외\n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x08xDuc42K1x",
        "outputId": "73a428a9-4547-40fb-93e8-c0e80315617a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2   35   16 ...    0    0    0]\n",
            " [   2   84 4243 ...    0    0    0]\n",
            " [   2  767   80 ...    0    0    0]\n",
            " ...\n",
            " [   2  913    4 ...    0    0    0]\n",
            " [   2    7  100 ...    0    0    0]\n",
            " [   2   29  203 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f94a9c45110>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-CfkfAG2NKh",
        "outputId": "56f41d97-aacb-4e96-ad82-da21ccc6a33b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = tensor[:, :-1] # 소스문장 생성 \n",
        "\n",
        "tgt_input = tensor[:, 1:]  # 타켓 문장 생성\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRos29QH2VcT",
        "outputId": "aec73805-c8d5-44d1-96c1-20c9869d49ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2  35  16  27 668  10  91  12   7  56 719   3   0   0   0   0   0   0\n",
            "   0]\n",
            "[ 35  16  27 668  10  91  12   7  56 719   3   0   0   0   0   0   0   0\n",
            "   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 객체 생성 및 전처리"
      ],
      "metadata": {
        "id": "J20YVOAUdQK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 14000개와, 여기 포함되지 않은 0:<pad>를 포함하여 14001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1an42ht2Zll",
        "outputId": "220deaa2-d8e3-4ba8-bc00-630af91ed691"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 19), dtype=tf.int32, name=None), TensorSpec(shape=(256, 19), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size는 몇 개의 관측치에 대한 예측을 하고, 레이블 값과 비교를 하는지를 설정하는 파라미터이다. batch_size가 256이면 전체 데이터에 대해 모두 예측한 뒤 실제 레이블 값과 비교한 후 가중치 갱신한다. batch_size가 128이면 128개 데이터에 대해 예측한 뒤 실제 레이블 값과 비교하며 가중치 갱신도 128번 발생한다. \\\n",
        "batch_size가 클수록 많은 데이터를 저장해두어야 하므로 용량이 커진다. 반면, batch_size가 작으면 학습은 촘촘하게 되지만, 계속 레이블과 비교하고, 가중치를 업데이트하는 과정을 거치면서 시간이 오래 걸린다. \\\n",
        "step은 Gradient Descent를 batch_size별로 계산하되, 전체 학습 데이터에 대해서 이를 반복해서 적용한 회수를 말한다. \\\n",
        " "
      ],
      "metadata": {
        "id": "fNR6UKMceJHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 분리"
      ],
      "metadata": {
        "id": "n8C8pW8WcCgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2)"
      ],
      "metadata": {
        "id": "KCK79RxN2bx-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 모델 생성"
      ],
      "metadata": {
        "id": "EaQQKYYucGan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear1 = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear1(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "wDpkKuNK2co7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습시키기"
      ],
      "metadata": {
        "id": "kKmylDsjcJ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size= 256\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "history = []\n",
        "epochs = 10\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = model.fit(dataset, \n",
        "          epochs=epochs,\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtnAfLaG2e_c",
        "outputId": "25a5dbd9-14e2-41db-cfec-e36d3bff3831"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "460/460 [==============================] - 191s 365ms/step - loss: 3.0929 - val_loss: 2.6599\n",
            "Epoch 2/10\n",
            "460/460 [==============================] - 167s 363ms/step - loss: 2.5897 - val_loss: 2.4487\n",
            "Epoch 3/10\n",
            "460/460 [==============================] - 167s 363ms/step - loss: 2.4337 - val_loss: 2.3043\n",
            "Epoch 4/10\n",
            "460/460 [==============================] - 167s 363ms/step - loss: 2.3102 - val_loss: 2.1745\n",
            "Epoch 5/10\n",
            "460/460 [==============================] - 167s 362ms/step - loss: 2.1934 - val_loss: 2.0493\n",
            "Epoch 6/10\n",
            "460/460 [==============================] - 167s 362ms/step - loss: 2.0753 - val_loss: 1.9251\n",
            "Epoch 7/10\n",
            "460/460 [==============================] - 167s 362ms/step - loss: 1.9558 - val_loss: 1.8010\n",
            "Epoch 8/10\n",
            "460/460 [==============================] - 167s 363ms/step - loss: 1.8348 - val_loss: 1.6725\n",
            "Epoch 9/10\n",
            "460/460 [==============================] - 167s 362ms/step - loss: 1.7124 - val_loss: 1.5522\n",
            "Epoch 10/10\n",
            "460/460 [==============================] - 167s 363ms/step - loss: 1.5907 - val_loss: 1.4283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문장 생성하고 사람이 평가하기"
      ],
      "metadata": {
        "id": "L3EHYM-5co4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "rnIDQ_EO2igj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 출력해보기"
      ],
      "metadata": {
        "id": "hfsFQNEsiWQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> anyway\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cZDU-2Ad2kLT",
        "outputId": "0fc38c9e-e220-40f8-9dc5-09113c21f1ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> anyway , i m gonna be a star <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 출처"
      ],
      "metadata": {
        "id": "A_KNV3mdcNr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ai-rtistic.com/2021/08/20/toy-project-lyricist/#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0 \\\n",
        "\n",
        "https://github.com/PEBpung/Aiffel/blob/master/Project/Exploration/E11.%20%EC%9E%91%EC%82%AC%EA%B0%80%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb \\\n",
        "\n",
        "https://sevillabk.github.io/1-batch-epoch/ \\\n",
        "\n",
        "https://bslife.tistory.com/73 \n",
        "\n"
      ],
      "metadata": {
        "id": "8Pl1PZV3dBDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고"
      ],
      "metadata": {
        "id": "aKA4_K1edrVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 객체를 만들고 모델에 적용하지 못해서 많은 시간을 끌었다. steps_per_epoch 값과 실제 step간의 차이를 재은님이 발견하셨고 enc_train, dec_train을 넣었던 자리를 dataset으로 바꾸니 val_loss가 줄어들게 됐다. \\\n",
        "그래서 batch_size, step에 대해 자세히 공부를 해보았고 내가 모델이 내가 원하는 데이터를 학습하고 있는지 확인하기 위해서 step을 확인해봐야겠다는 insight를 얻게 됐다. \\\n",
        "데이터가 잘 학습하고 있는지를 확인한 후에 하이퍼 파라미터(embedding_size, hidden_size)와 내가 작성한 모델을 확인해 봐야겠다는 insight 역시 얻게 됐다.\n"
      ],
      "metadata": {
        "id": "U-Tq34FIdswj"
      }
    }
  ]
}