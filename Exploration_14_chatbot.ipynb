{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422092d9",
   "metadata": {},
   "source": [
    "# 0. 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ae4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e4621",
   "metadata": {},
   "source": [
    "# 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3439346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe7a1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee591f59",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ceb1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이에 공백을 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (한글, 알파벳, 숫자 \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백으로 대체합니다.\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e3839",
   "metadata": {},
   "source": [
    "- 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbaa72",
   "metadata": {},
   "source": [
    "| 메타문자 | 기능 | 설명 |\n",
    "|---|---|---|\n",
    "| . | 문자 | 1개의 문자와 일치한다. 단일행 모드에서는 새줄 문자를 제외한한다. |\n",
    "| [] | 문자클래스 | \"[\"과 \"]\" 사이의 문자 중 하나를 선택한다. \"¦\"를 여러 개 쓴 것과 같은 의미이다.  예를 들면 [abc]d는 ad, bd, cd를 뜻한다.  또한, \"-\" 기호와 함께 쓰면 범위를 지정할 수 있다. \"[a-z]\"는 a부터 z까지 중 하나, \"[1-9]\"는 1부터 9까지 중의 하나를 의미한다. |\n",
    "| [^ ] | 부정 | 문자 클래스 안의 문자를 제외한 나머지를 선택한다. 예를 들면 [^abc]d는 ad, bd, cd는 포함하지 않고 ed, fd 등을 포함한다. [^a-z]는 알파벳 소문자로 시작하지 않는 모든 문자를 의미한다. |\n",
    "| ^ | 처음 | 문자열이나 행의 처음을 의미한다. |\n",
    "| $ | 끝 | 문자열이나 행의 끝을 의미한다. |\n",
    "| ( ) | 하위식 | 여러 식을 하나로 묶을 수 있다. \"abc¦adc\"와 \"a(b¦d)c\"는 같은 의미를 가진다. |\n",
    "| \\n | 일치하는 n번째 패턴 | 일치하는 패턴들 중 n번째를 선택하며, 여기에서 n은 1에서 9 중 하나가 올 수 있다. |\n",
    "| * | 0회 이상 | 0개 이상의 문자를 포함한다. \"a*b\"는 \"b\", \"ab\", \"aab\", \"aaab\"를 포함한다. |\n",
    "| {m, n} | m회 이상 n회 이하 | \"a{1,3}b\"는 \"ab\", \"aab\", \"aaab\"를 포함하지만, \"b\"나 \"aaaab\"는 포함하지 않는다. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4580dfc",
   "metadata": {},
   "source": [
    "| 메타문자 | 기능 | 설명 |\n",
    "|---|---|---|\n",
    "| ? | 0 또는 1회 | \"a?b\"는 \"b\", \"ab\"를 포함한다. |\n",
    "| + | 1회 이상 | \"a+b\"는 \"ab\", \"aab\", \"aaab\"를 포함하지만 \"b\"는 포함하지 않는다.  |\n",
    "| ¦ | 선택 | 여러 식 중에서 하나를 선택한다. 예를 들어, \"abc¦adc\"는 abc와 adc 문자열을 모두 포함한다. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0d7bc",
   "metadata": {},
   "source": [
    "| ASCII | 설명 |\n",
    "|---|---|\n",
    "| [A-Za-z0-9] | 영숫자 |\n",
    "| [A-Za-z0-9_] | 영숫자 + \"_\" |\n",
    "| [^A-Za-z0-9_] | 낱말이 아닌 문자 |\n",
    "| [A-Za-z] | 알파벳 문자 |\n",
    "| [ \\t] | 공백과 탭 |\n",
    "| (?<=\\W)(?=\\w)\\|(?<=\\w)(?=\\W) | 낱말 경계 |\n",
    "| [\\x00-\\x1F\\x7F] | 제어 문자 |\n",
    "| [0-9] | 숫자 |\n",
    "| [^0-9] | 숫자가 아닌 문자 |\n",
    "| [\\x21-\\x7E] | 보이는 문자 |\n",
    "| [a-z] | 소문자 |\n",
    "| [\\x20-\\x7E] | 보이는 문자 및 공백 문자 |\n",
    "| [][!\"#$%&'()*+,./:;<=>?@\\^_`{\\|}~-] | 구두점 |\n",
    "| [ \\t\\r\\n\\v\\f] | 공백 문자 |\n",
    "| [^ \\t\\r\\n\\v\\f] | 공백이 아닌 모든 문자 |\n",
    "| [A-Z] | 대문자 |\n",
    "| [A-Fa-f0-9] | 16진수 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039e84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "    inputs, outputs = [], []\n",
    "    for i in range(len(data)):\n",
    "        # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
    "        inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
    "        outputs.append(preprocess_sentence(data['A'].values[i]))\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc142a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fe061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 1번째 질문 샘플: 12시 땡 !\n",
      "전처리 후의 1번째 답변 샘플: 하루가 또 가네요 .\n",
      "전처리 후의 2번째 질문 샘플: 1지망 학교 떨어졌어\n",
      "전처리 후의 2번째 답변 샘플: 위로해 드립니다 .\n",
      "전처리 후의 3번째 질문 샘플: 3박4일 놀러가고 싶다\n",
      "전처리 후의 3번째 답변 샘플: 여행은 언제나 좋죠 .\n",
      "전처리 후의 4번째 질문 샘플: 3박4일 정도 놀러가고 싶다\n",
      "전처리 후의 4번째 답변 샘플: 여행은 언제나 좋죠 .\n",
      "전처리 후의 5번째 질문 샘플: ppl 심하네\n",
      "전처리 후의 5번째 답변 샘플: 눈살이 찌푸려지죠 .\n",
      "전처리 후의 6번째 질문 샘플: sd카드 망가졌어\n",
      "전처리 후의 6번째 답변 샘플: 다시 새로 사는 게 마음 편해요 .\n",
      "전처리 후의 7번째 질문 샘플: sd카드 안돼\n",
      "전처리 후의 7번째 답변 샘플: 다시 새로 사는 게 마음 편해요 .\n",
      "전처리 후의 8번째 질문 샘플: sns 맞팔 왜 안하지\n",
      "전처리 후의 8번째 답변 샘플: 잘 모르고 있을 수도 있어요 .\n",
      "전처리 후의 9번째 질문 샘플: sns 시간낭비인 거 아는데 매일 하는 중\n",
      "전처리 후의 9번째 답변 샘플: 시간을 정하고 해보세요 .\n",
      "전처리 후의 10번째 질문 샘플: sns 시간낭비인데 자꾸 보게됨\n",
      "전처리 후의 10번째 답변 샘플: 시간을 정하고 해보세요 .\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('전처리 후의 {}번째 질문 샘플: {}'.format(i+1, questions[i]))\n",
    "    print('전처리 후의 {}번째 답변 샘플: {}'.format(i+1, answers[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937b729",
   "metadata": {},
   "source": [
    "# 3. 병렬 데이터 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753cc4c",
   "metadata": {},
   "source": [
    "## 3-1. 단어장(Vocabulary) 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4546c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1157dfc",
   "metadata": {},
   "source": [
    "| Args | 설명 |\n",
    "|---|---|\n",
    "| Corpus_generator | 하위 단어가 생성될 str을 생성하는 생성기. |\n",
    "| target_vocab_size | int, 생성할 어휘의 대략적인 크기. |\n",
    "| max_subword_length | int, 하위 단어의 최대 길이. 메모리와 컴퓨팅은 가장 긴 토큰의 길이에 따라 2차적으로 확장됩니다. |\n",
    "| max_corpus_chars | int, 하위 단어 어휘를 구축하기 위해 corpus_generator에서 소비할 최대 문자 수입니다 |\n",
    "| reserved_tokens | list(str), 항상 전체 토큰으로 처리되고 분할되지 않는 토큰 목록입니다. 여기에는 영숫자와 영숫자가 아닌 문자(예: \"\")가 혼합되어 있어야 하며 밑줄로 끝나지 않아야 합니다. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7961ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64fa25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [21798]\n",
      "END_TOKEN의 번호 : [21799]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1756edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21800\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4693d",
   "metadata": {},
   "source": [
    "## 3-2. 정수 인코딩 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c41e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5191, 1376, 2629]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [1306, 8698, 6, 6198, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2100d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문의 최소 길이 : 1\n",
      "질문의 최대 길이 : 16\n",
      "질문의 평균 길이 : 3.9378330373001775\n",
      "답변의 최소 길이 : 1\n",
      "답변의 최대 길이 : 24\n",
      "답변의 평균 길이 : 4.716146494121627\n"
     ]
    }
   ],
   "source": [
    "# 질문과 답변의 길이 살펴보기\n",
    "question_len = [len(s.split()) for s in questions]\n",
    "answer_len = [len(s.split()) for s in answers]\n",
    "\n",
    "print('질문의 최소 길이 : {}'.format(np.min(question_len)))\n",
    "print('질문의 최대 길이 : {}'.format(np.max(question_len)))\n",
    "print('질문의 평균 길이 : {}'.format(np.mean(question_len)))\n",
    "print('답변의 최소 길이 : {}'.format(np.min(answer_len)))\n",
    "print('답변의 최대 길이 : {}'.format(np.max(answer_len)))\n",
    "print('답변의 평균 길이 : {}'.format(np.mean(answer_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c11e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 15\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29e965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18565ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 21800\n",
      "필터링 후의 질문 샘플 개수: 11780\n",
      "필터링 후의 답변 샘플 개수: 11780\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64803f24",
   "metadata": {},
   "source": [
    "## 3-3. 교사 강요(Force Teaching) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a41e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d483632",
   "metadata": {},
   "source": [
    "# 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55e565",
   "metadata": {},
   "source": [
    "## 함수 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658546b",
   "metadata": {},
   "source": [
    "### 포지셔닝 인코딩 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f39ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1) # axis=-1이면 낮은 차원으로 concat한다는 의미이다.\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3983ae",
   "metadata": {},
   "source": [
    "- tf.cast\n",
    "\n",
    "괄호 안에는 x, dtype, name 인수가 있습니다.\n",
    "\n",
    "tf.cast()는 배열의 dtype을 변경해 줍니다.\n",
    "\n",
    "x에는 변경될 배열이 들어갑니다.\n",
    "\n",
    "dtype에는 변경할 dtype이 들어갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15baab0",
   "metadata": {},
   "source": [
    "- 포지셔닝 인코딩\n",
    "\n",
    "어텐션 레이어는 입력을 순차적인 순서가 없는 벡터 세트로 봅니다. 이 모델에는 순환 또는 컨볼루션 레이어도 포함되어 있지 않습니다. 이 때문에 \"위치 인코딩\"이 추가되어 모델에 문장에서 토큰의 상대적 위치에 대한 정보를 제공합니다.\n",
    "\n",
    "위치 인코딩 벡터가 임베딩 벡터에 추가됩니다. 임베딩은 비슷한 의미를 가진 토큰이 서로 더 가까워지는 d차원 공간의 토큰을 나타냅니다. 그러나 임베딩은 문장에서 토큰의 상대적 위치를 인코딩하지 않습니다. 따라서 위치 인코딩을 추가한 후 토큰은 d차원 공간 에서 의미의 유사성과 문장에서의 위치에 따라 서로 더 가까워집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd8015",
   "metadata": {},
   "source": [
    "### 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e7e49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"어텐션 가중치를 계산. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax 적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "  \n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc816f7d",
   "metadata": {},
   "source": [
    "softmax normalization이 K에 대해 수행되면 해당 값에 따라 Q에 부여되는 중요도가 결정됩니다.\n",
    "\n",
    "출력은 주의 가중치와 V(값) 벡터의 곱을 나타냅니다. 이렇게 하면 집중하려는 토큰이 있는 그대로 유지되고 관련 없는 토큰이 플러시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "870c3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다.\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다.\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # final linear layer\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a408e93",
   "metadata": {},
   "source": [
    "### 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7549e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69523ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c8676",
   "metadata": {},
   "source": [
    "시퀀스 배치에서 모든 패드 토큰을 마스킹합니다. 모델이 패딩을 입력으로 취급하지 않도록 합니다. 마스크는 패드 값 0 이 있는 위치를 나타냅니다. 해당 위치에서 1 을 출력하고 그렇지 않으면 0 을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df0d18",
   "metadata": {},
   "source": [
    "### 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2004997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "\t# 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a57d6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "\t# 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855491f5",
   "metadata": {},
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c011922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afb25acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "\t# 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "\t# 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "\t# 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "\t# Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7095778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "\t# 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88ae06",
   "metadata": {},
   "source": [
    "## 4-2. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b818e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    6635008     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    7162368     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 21800)  5602600     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,399,976\n",
      "Trainable params: 19,399,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d5855",
   "metadata": {},
   "source": [
    "## 4-3. 손실함수(Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbecd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d39dd7",
   "metadata": {},
   "source": [
    "## 4-4. 커스텀된 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3b7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89eaab4",
   "metadata": {},
   "source": [
    "## 4-5. 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9817f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21727ea8",
   "metadata": {},
   "source": [
    "## 4-6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf4ad330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "185/185 [==============================] - 16s 54ms/step - loss: 3.7929 - accuracy: 0.0568\n",
      "Epoch 2/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 3.0020 - accuracy: 0.1346\n",
      "Epoch 3/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 2.3578 - accuracy: 0.1392\n",
      "Epoch 4/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 2.1228 - accuracy: 0.1443\n",
      "Epoch 5/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.9790 - accuracy: 0.1519\n",
      "Epoch 6/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.8509 - accuracy: 0.1590\n",
      "Epoch 7/20\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 1.7098 - accuracy: 0.1700\n",
      "Epoch 8/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.5484 - accuracy: 0.1853\n",
      "Epoch 9/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.3695 - accuracy: 0.2061\n",
      "Epoch 10/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 1.1778 - accuracy: 0.2298\n",
      "Epoch 11/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.9818 - accuracy: 0.2529\n",
      "Epoch 12/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.7909 - accuracy: 0.2768\n",
      "Epoch 13/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.6133 - accuracy: 0.3015\n",
      "Epoch 14/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.4600 - accuracy: 0.3237\n",
      "Epoch 15/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.3319 - accuracy: 0.3456\n",
      "Epoch 16/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.2296 - accuracy: 0.3640\n",
      "Epoch 17/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1611 - accuracy: 0.3761\n",
      "Epoch 18/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.1181 - accuracy: 0.3837\n",
      "Epoch 19/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0934 - accuracy: 0.3873\n",
      "Epoch 20/20\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0745 - accuracy: 0.3905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0904e9970>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65bcae",
   "metadata": {},
   "source": [
    "# 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef0f82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93a93eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd59292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕\n",
      "출력 : 안녕하세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요 .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('안녕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0338fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 밥 먹었어?\n",
      "출력 : 저는 배터리가 밥이예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 배터리가 밥이예요 .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('밥 먹었어?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a773638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 심심해\n",
      "출력 : 친구들과 연락해보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'친구들과 연락해보세요 .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('심심해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed5d120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 누구세요?\n",
      "출력 : 저는 위로해드리는 로봇이에요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 위로해드리는 로봇이에요 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('누구세요?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9e3c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 놀러 가고 싶다\n",
      "출력 : 저도 참 힘들어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 참 힘들어요 .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('놀러 가고 싶다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c64f02",
   "metadata": {},
   "source": [
    "# 출처\n",
    "https://github.com/kec0130/AIFFEL-project/blob/main/exploration/E15_chatbot_kr_pjt.ipynb\n",
    "\n",
    "https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder\n",
    "\n",
    "https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%ED%91%9C%ED%98%84%EC%8B%9D\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e0efc",
   "metadata": {},
   "source": [
    "# 회고\n",
    "NLP의 끝판왕인 Transformer를 경험했다. 이때까지 배운 NLP를 전부 적용해서 만든 아키텍처인거 같다. NLP하면서 가장 재밌게 한 Exploration이다. Transformer를 나중에 논문을 읽고 내가 직접 구현해봐야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
